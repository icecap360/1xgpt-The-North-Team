{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os, copy\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model conversion utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_dict_to_vector(state_dict, remove_keys=[]):\n",
    "    shared_state_dict = copy.deepcopy(state_dict)\n",
    "    for key in remove_keys:\n",
    "        if key in shared_state_dict:\n",
    "            del shared_state_dict[key]\n",
    "    sorted_shared_state_dict = OrderedDict(sorted(shared_state_dict.items()))\n",
    "    return torch.nn.utils.parameters_to_vector(\n",
    "        [value.reshape(-1) for key, value in sorted_shared_state_dict.items()]\n",
    "    )\n",
    "\n",
    "\n",
    "def vector_to_state_dict(vector, state_dict, remove_keys=[]):\n",
    "    # create a reference dict to define the order of the vector\n",
    "    reference_dict = copy.deepcopy(state_dict)\n",
    "    for key in remove_keys:\n",
    "        if key in reference_dict:\n",
    "            del reference_dict[key]\n",
    "    sorted_reference_dict = OrderedDict(sorted(reference_dict.items()))\n",
    "\n",
    "    # create a shared state dict using the refence dict\n",
    "    torch.nn.utils.vector_to_parameters(vector, sorted_reference_dict.values())\n",
    "\n",
    "    # add back the encoder and decoder embedding weights.\n",
    "    if \"transformer.shared.weight\" in sorted_reference_dict:\n",
    "        for key in remove_keys:\n",
    "            sorted_reference_dict[key] = sorted_reference_dict[\n",
    "                \"transformer.shared.weight\"\n",
    "            ]\n",
    "    return sorted_reference_dict\n",
    "\n",
    "\n",
    "def add_ptm_to_tv(tv_dict, ptm_dict):\n",
    "    assert set(tv_dict.keys()) == set(\n",
    "        ptm_dict.keys()\n",
    "    ), \"Differing parameter names in models.\"\n",
    "    final_dict = copy.deepcopy(tv_dict)\n",
    "    for k, v in ptm_dict.items():\n",
    "        final_dict[k] = tv_dict[k] + v\n",
    "    return final_dict\n",
    "\n",
    "\n",
    "def check_parameterNamesMatch(checkpoints):\n",
    "    parameter_names = set(checkpoints[0].keys())\n",
    "\n",
    "    if len(checkpoints) >= 2:\n",
    "        # raise ValueError(\"Number of models is less than 2.\")\n",
    "        for checkpoint in checkpoints[1:]:\n",
    "            current_parameterNames = set(checkpoint.keys())\n",
    "            if current_parameterNames != parameter_names:\n",
    "                raise ValueError(\n",
    "                    \"Differing parameter names in models. \"\n",
    "                    f\"The different parameters are {parameter_names.symmetric_difference(current_parameterNames)}\"\n",
    "                )\n",
    "\n",
    "def check_state_dicts_equal(state_dict1, state_dict2):\n",
    "    if set(state_dict1.keys()) != set(state_dict2.keys()):\n",
    "        return False\n",
    "\n",
    "    for key in state_dict1.keys():\n",
    "        if not torch.equal(state_dict1[key], state_dict2[key]):\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load base model and the Finetuned Models to merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/playpen/prateek/anaconda3/envs/geocl/lib/python3.7/site-packages/transformers/models/t5/tokenization_t5_fast.py:166: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\").to(\"cpu\").state_dict()\n",
    "\n",
    "# some keys are tied together so they need to be removed,\n",
    "# for example in this case, we will only keep the shared.weight and remove the other two.\n",
    "assert (model['shared.weight'] - model['encoder.embed_tokens.weight']).sum() == 0\n",
    "assert (model['shared.weight'] - model['decoder.embed_tokens.weight']).sum() == 0\n",
    "\n",
    "# Load all the models to merge\n",
    "model_rte = AutoModelForSeq2SeqLM.from_pretrained(\"PavanNeerudu/t5-base-finetuned-rte\").to(\"cpu\").state_dict()\n",
    "model_mnli = AutoModelForSeq2SeqLM.from_pretrained(\"PavanNeerudu/t5-base-finetuned-mnli\").to(\"cpu\").state_dict()\n",
    "model_sst2 = AutoModelForSeq2SeqLM.from_pretrained(\"PavanNeerudu/t5-base-finetuned-sst2\").to(\"cpu\").state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattening out Checkpoints\n"
     ]
    }
   ],
   "source": [
    "ft_checks = [model_rte, model_mnli, model_sst2]\n",
    "ptm_check = model\n",
    "\n",
    "# check if all checkpoints have the same paramters.\n",
    "check_parameterNamesMatch(ft_checks + [ptm_check])\n",
    "\n",
    "# Removing the two keys from state dict when creating the task vector.\n",
    "# Basically these keys are not involved in the global operations like the computation of topk.\n",
    "remove_keys = [\n",
    "    \"transformer.encoder.embed_tokens.weight\",\n",
    "    \"transformer.decoder.embed_tokens.weight\",\n",
    "]\n",
    "\n",
    "print(f\"Flattening out Checkpoints\")\n",
    "flat_ft = torch.vstack(\n",
    "    [state_dict_to_vector(check, remove_keys) for check in ft_checks]\n",
    ")\n",
    "flat_ptm = state_dict_to_vector(ptm_check, remove_keys)\n",
    "\n",
    "# Creating Task vectors\n",
    "tv_flat_checks = flat_ft - flat_ptm\n",
    "\n",
    "# check if the vectorized state dicts can be converted back to the original state dicts\n",
    "# covnert back the flat task vectors to state dict and see if the original and converted sd's are equal\n",
    "assert check_state_dicts_equal(\n",
    "        vector_to_state_dict(flat_ptm, ptm_check, remove_keys), ptm_check\n",
    "    )\n",
    "assert all(\n",
    "    [\n",
    "        check_state_dicts_equal(\n",
    "            vector_to_state_dict(flat_ft[i], ptm_check, remove_keys), ft_checks[i]\n",
    "        )\n",
    "        for i in range(len(ft_checks))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TIES MERGING UTILS\n",
    "\n",
    "def topk_values_mask(M, K=0.7, return_mask=False):\n",
    "    if K > 1:\n",
    "        K /= 100\n",
    "\n",
    "    original_shape = M.shape\n",
    "    if M.dim() == 1:\n",
    "        M = M.unsqueeze(0)\n",
    "\n",
    "    n, d = M.shape\n",
    "    k = int(d * K)\n",
    "    k = d - k  # Keep top k elements instead of bottom k elements\n",
    "\n",
    "    # Find the k-th smallest element by magnitude for each row\n",
    "    kth_values, _ = M.abs().kthvalue(k, dim=1, keepdim=True)\n",
    "    # Create a mask tensor with True for the top k elements in each row\n",
    "    mask = M.abs() >= kth_values\n",
    "    final_mask = mask.squeeze() if original_shape == M.squeeze().shape else mask\n",
    "\n",
    "    if return_mask:\n",
    "        return M * final_mask, final_mask.float().mean(dim=1), final_mask\n",
    "    return M * final_mask, final_mask.float().mean(dim=1)\n",
    "\n",
    "\n",
    "def resolve_zero_signs(sign_to_mult, method=\"majority\"):\n",
    "    majority_sign = torch.sign(sign_to_mult.sum())\n",
    "\n",
    "    if method == \"majority\":\n",
    "        sign_to_mult[sign_to_mult == 0] = majority_sign\n",
    "    elif method == \"minority\":\n",
    "        sign_to_mult[sign_to_mult == 0] = -1 * majority_sign\n",
    "    return sign_to_mult\n",
    "\n",
    "\n",
    "def resolve_sign(Tensor):\n",
    "    sign_to_mult = torch.sign(Tensor.sum(dim=0))\n",
    "    sign_to_mult = resolve_zero_signs(sign_to_mult, \"majority\")\n",
    "    return sign_to_mult\n",
    "\n",
    "\n",
    "def disjoint_merge(Tensor, merge_func, sign_to_mult):\n",
    "\n",
    "    merge_func = merge_func.split(\"-\")[-1]\n",
    "\n",
    "    # If sign is provided then we select the corresponding entries and aggregate.\n",
    "    if sign_to_mult is not None:\n",
    "        rows_to_keep = torch.where(\n",
    "            sign_to_mult.unsqueeze(0) > 0, Tensor > 0, Tensor < 0\n",
    "        )\n",
    "        selected_entries = Tensor * rows_to_keep\n",
    "    # Else we select all non-zero entries and aggregate.\n",
    "    else:\n",
    "        rows_to_keep = Tensor != 0\n",
    "        selected_entries = Tensor * rows_to_keep\n",
    "\n",
    "    if merge_func == \"mean\":\n",
    "        non_zero_counts = (selected_entries != 0).sum(dim=0).float()\n",
    "        disjoint_aggs = torch.sum(selected_entries, dim=0) / torch.clamp(\n",
    "            non_zero_counts, min=1\n",
    "        )\n",
    "    elif merge_func == \"sum\":\n",
    "        disjoint_aggs = torch.sum(selected_entries, dim=0)\n",
    "    elif merge_func == \"max\":\n",
    "        disjoint_aggs = selected_entries.abs().max(dim=0)[0]\n",
    "        disjoint_aggs *= sign_to_mult\n",
    "    else:\n",
    "        raise ValueError(f\"Merge method {merge_func} is not defined.\")\n",
    "\n",
    "    return disjoint_aggs\n",
    "\n",
    "\n",
    "def ties_merging(\n",
    "    flat_task_checks,\n",
    "    reset_thresh=None,\n",
    "    merge_func=\"\",\n",
    "):\n",
    "    all_checks = flat_task_checks.clone()\n",
    "    updated_checks, *_ = topk_values_mask(\n",
    "        all_checks, K=reset_thresh, return_mask=False\n",
    "    )\n",
    "    print(f\"RESOLVING SIGN\")\n",
    "    final_signs = resolve_sign(updated_checks)\n",
    "    assert final_signs is not None\n",
    "    \n",
    "    print(f\"Disjoint AGGREGATION: {merge_func}\")\n",
    "    merged_tv = disjoint_merge(updated_checks, merge_func, final_signs)\n",
    "    \n",
    "    return merged_tv\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TIES Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESOLVING SIGN\n",
      "Disjoint AGGREGATION: dis-mean\n"
     ]
    }
   ],
   "source": [
    "# TIES Merging example\n",
    "K = 20\n",
    "merge_func = \"dis-mean\"\n",
    "lamda = 1\n",
    "\n",
    "# return merged flat task vector\n",
    "merged_tv = ties_merging(\n",
    "    tv_flat_checks,\n",
    "    reset_thresh=K,\n",
    "    merge_func=merge_func,\n",
    ")\n",
    "\n",
    "# add back the PTM to the flat merged task vector\n",
    "merged_check = flat_ptm + lamda * merged_tv\n",
    "\n",
    "# convert the flat merged checkpoint to a state dict\n",
    "merged_state_dict = vector_to_state_dict(\n",
    "    merged_check, ptm_check, remove_keys=remove_keys\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Vector Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK VECTOR MERGING UTILS\n",
    "\n",
    "def aggregate(T, agg_type, dim=0):\n",
    "    if agg_type == \"mean\":\n",
    "        result = torch.mean(T, dim=dim)\n",
    "    elif agg_type == \"sum\":\n",
    "        result = torch.sum(T, dim=dim)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid agg_type: %s\" % agg_type)\n",
    "\n",
    "    return result\n",
    "\n",
    "def tv_merging(tv_flat_checks):\n",
    "    \"\"\"Merging by creating and scaling Task Vectors\"\"\"\n",
    "    all_checks = tv_flat_checks.clone()\n",
    "    tv_merged_check = aggregate(all_checks, \"sum\")\n",
    "    return tv_merged_check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task Vector Merging example\n",
    "lamda = 0.4\n",
    "\n",
    "merged_tv = tv_merging(tv_flat_checks)\n",
    "merged_check = flat_ptm + lamda * merged_tv\n",
    "merged_state_dict = vector_to_state_dict(\n",
    "    merged_check, ptm_check, remove_keys=remove_keys\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geocl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61f278d9a3c8a89652052c2eab52c1c01c9f296af9c9c468294bdec07b749a1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
